---
---

@inproceedings{raja2025idiosyncratic,
  author = {Vishnu Raja and Adithya V. Ganesan and Anand Syamkumar and Ritwik Banerjee and H. A. Schwartz},
  title = {{Idiosyncratic Versus Normative Modeling of Atypical Speech Recognition: Dysarthric Case Studies}},
  booktitle = {Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year = {2025},
  month = {November},
  abstract = {State-of-the-art automatic speech recognition (ASR) models like Whisper, perform poorly on atypical speech, such as that produced by individuals with dysarthria. Past works for atypical speech have mostly investigated fully personalized (or idiosyncratic) models, but modeling strategies that can both generalize and handle idiosyncracy could be more effective for capturing atypical speech. To investigate this, we compare four strategies: (a) *normative* models trained on typical speech (no personalization), (b) idiosyncratic models completely personalized to individuals, (c) dysarthric-normative models trained on other dysarthric speakers, and  (d) dysarthric-idiosyncratic models which combine strategies by first modeling normative patterns before adapting to individual speech. In this case study, we find the dysarthric-idiosyncratic model performs better than idiosyncratic approach while requiring less than half as much personalized data (36.43 WER with 128 train size vs 36.99 with 256). Further, we found that tuning the speech encoder alone (as opposed to the LM decoder) yielded the best results reducing word error rate from 71\% to 32\% on average. Our findings highlight the value of leveraging both normative (cross-speaker) and idiosyncratic (speaker-specific) patterns to improve ASR for underrepresented speech populations.},
  bibtex_show = {true},
  pdf = {raja2025idiosyncratic.pdf},
  preview = {true},
  %google_scholar_id = {}
}

@inproceedings{tamrakar2025harnessing,
  title = {{Harnessing Language Models to Analyze Android App Permission Fidelity}},
  author = {Tamrakar, Yunik and Myers, Ethan and Banerjee, Ritwik and De Carli, Lorenzo and Ray, Indrakshi},
  year = {2025},
  booktitle = {Proceedings of the 22nd Annual International Conference on Privacy, Security, and Trust},
  series = {PST},
  address = {Fredericton, Canada},
  month = {August},
  publisher = {IEEE},
  abstract={Android’s vast app ecosystem (over 2 million apps) poses significant privacy risks, as current methods for inferring permissions from descriptions -- keyword matching, traditional natural language processing (NLP), and recurrent neural networks (RNNs) -- struggle with accurate inference due to imprecise, ambiguous, or incomplete natural language descriptions. This gap undermines regulatory transparency and user trust, necessitating tools that reconcile stated functionality with actual data practices. We demonstrate that large language models like GPT-4o, applied in a zero-shot inference setting, leverage contextual reasoning to infer permissions competitively, while fine-tuned encoders (BERT, BART) surpass state-of-the-art performance when trained on minimally annotated datasets augmented with paraphrases, achieving 50–70\% gains in weighted and macro $F_1$ scores. By enabling precise permission auditing with reduced annotation costs, our work advances scalable, adaptable solutions for privacy compliance across resource-constrained and high-stakes environments.},
  bibtex_show ={true},
  pdf = {tamrakar2025harnessing.pdf},
  selected = {false},
  %google_scholar_id = {todo},
  preview = {true},
  slides = {todo}
}

@article{wang2025natural,
  author = {Chenlu Wang and Ritwik Banerjee and Harry Kuperstein and Hamza Malick and Ruqiyya Bano and Robin L. Cunningham and Hira Tahir and Priyal Sakhuja and Janos Hajagos and Farrukh M. Koraishy},
  title = {{Natural language processing for kidney ultrasound analysis: correlating imaging reports with chronic kidney disease diagnosis}},
  journal = {Renal Failure},
  volume = {47},
  number = {1},
  pages = {2539938},
  year = {2025},
  month = {August},
  publisher = {Taylor \& Francis},
  doi = {10.1080/0886022X.2025.2539938},
  url = {https://doi.org/10.1080/0886022X.2025.2539938},
  abstract = {Natural language processing (NLP) has been used to analyze unstructured imaging report data, yet its application in identifying chronic kidney disease (CKD) features from kidney ultrasound reports remains unexplored. In a single-center pilot study, we analyzed 1,068 kidney ultrasound reports using NLP techniques. To identify kidney echogenicity as either "normal" or "increased," we used two methods: one that looks at individual words and another that analyzes full sentences. Kidney length was identified as "small" if its length was below the 10th percentile. Nephrologists reviewed 100 randomly selected reports to create the reference standard (ground truth) for initial model training followed by model validation on an independent set of 100 reports. The word-level NLP model outperformed the sentence-level approach in classifying increased echogenicity (accuracy: 0.96 vs. 0.89 for the left kidney; 0.97 vs. 0.92 for the right kidney). This model was then applied to the full dataset to assess associations with CKD. Multivariable logistic regression identified bilaterally increased echogenicity as the strongest predictor of CKD (odds ratio [OR] = 7.642, 95\% confidence interval [CI]: 4.887–11.949; p < 0.0001), followed by bilaterally small kidneys (OR = 4.981 [1.522, 16.300]; p=0.008). Among individuals without CKD, those with bilaterally increased echogenicity had significantly lower kidney function than those with normal echogenicity. State-of-the-art NLP models can accurately extract CKD-related features from ultrasound reports, with the potential of providing a scalable tool for early detection and risk stratification. Future research should focus on validating these models across different healthcare systems.},
  bibtex_show ={true},
  pdf = {wang2025natural-renalfailure.pdf},
  html = {https://pmc.ncbi.nlm.nih.gov/articles/PMC12322987/},
  selected = {false},
  %google_scholar_id = {D_sINldO8mEC},
  preview = {true},
}


@inproceedings{gupta2025scire,
    author = {Gupta, Harsh P. and Banerjee, Ritwik},
    title = {{SCIRE at BioASQ 2025: LLM-Driven Biomedical Named Entity Recognition for GutBrainIE 2025}},
    booktitle = {Working Notes of CLEF 2025 - Conference and Labs of the Evaluation Forum},
    series = {{CEUR} Workshop Proceedings},
    pages = {281--291},
    publisher = {CEUR-WS.org},
    address = {Madrid, Spain},
    year = {2025},
    abstract={n recent years, we have witnessed the rise of powerful Large Language Models (LLMs) and their flexibility in accomplishing a wide range of NLP tasks, often achieving state-of-the-art (SOTA) accuracy. However for Named Entity Recognition (NER), there is a specific need for token-level class assignments and bidirectional context, both preceding and following a token, to understand its role. As a result, bidirectional encoder-style transformer models (BERT-like models) have been the standard approach. However, fine-tuning these models on available datasets often faces the bottleneck of limited training data. In this paper, we propose an alternative approach that leverages the extensive knowledge base of decoder-style Transformer models. These modern LLMs are typically trained on vast amounts of text, which enables them to overcome the challenge of limited labeled data. Instead of training from scratch, we focus on aligning the responses of these LLMs to suit NER. To this end, we use two methods:(i) few-shot prompting, and (ii) fine-tuning on available examples. Our findings indicate that fine-tuning significantly outperforms prompting for biomedical NER, effectively aligning LLM outputs to the desired task outputs. Additionally, we propose an algorithm to parse the output of the LLM to extract relevant entities, their labels, and their start and end indices.},
    url={https://www.dei.unipd.it/~faggioli/temp/clef2025/paper_18.pdf},
    %google_scholar_id = {q3oQSFYPqjQC},
    bibtex_show={true},
    pdf={gupta2025scire-clef.pdf},
    code={https://github.com/hpgupt/GutBrainIE-CLEF25},
    selected={false},
    preview={true}
}


@inproceedings{thapliyal2025,
    author = {Thapliyal, Parth Manish and Chavan, Ritesh Sunil and Samridh, Samridh and Zuo, Chaoyuan and Banerjee, Ritwik},
    title = {{SCIRE at CheckThat! 2025: Bridging social media, scientific discourse, and scientific literature}},
    booktitle = {Working Notes of CLEF 2025 - Conference and Labs of the Evaluation Forum},
    series = {{CEUR} Workshop Proceedings},
    pages = {1256--1264},
    year = {2025},
    publisher = {CEUR-WS.org},
    address = {Madrid, Spain},
    abstract = {The increasing prominence of scientific discourse on social media platforms presents both unprecedented opportunities for public engagement and significant risks of misinformation. While scientific claims, references to publications, and mentions of research entities proliferate rapidly, current platforms lack robust mechanisms to validate their veracity or trace implicit sources. Manual identification and sourcing of such content is impractical at scale, and although computational methods exist for generic fact-checking or citation retrieval, they often fail to address the unique challenges of noisy, abbreviated social media language – particularly the detection of nuanced scientific discourse and the retrieval of publications from implicit, non-URL references. In this paper, we propose a unified framework tackling two critical tasks: (1) detection of scientific web discourse, where we identify tweets containing scientific claims, references or research entities, using a combination of natural language augmentation and supervised learning; and (2) source retrieval for scientific claims, employing a two-stage dense retrieval and re-ranking pipeline to link implicit mentions of sources to their actual publications from candidate pools. Our multi-stage architecture first filters and classifies scientific content, then prioritizes and resolves latent citations. Evaluations on a curated dataset provided by the CLEF-2025 CheckThat! Lab demonstrate the effectiveness of our approach, achieving significant improvements across both tasks. This work provides essential tools for automating scientific credibility assessment and aiding the verification of scientific information in online ecosystems.},
    url = {https://ceur-ws.org/Vol-4038/paper_99.pdf},
    bibtex_show = {true},
    pdf = {thapliyal2025scire-clef.pdf},
    selected={false},
    preview={true},
    %google_scholar_id = {BrmTIyaxlBUC},
}

@inproceedings{wang2025class,
    title = {{Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks}},
    author = {Wang, Chenlu and Lyu, Weimin and Banerjee, Ritwik},
    editor = {Che, Wanxiang and Nabende, Joyce and Shutova, Ekaterina and Pilehvar, Mohammad Taher},
    booktitle = {Proceedings of the 63rd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
    month = {July},
    year = {2025},
    address = {Vienna, Austria},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2025.acl-long.1424/},
    doi = {10.18653/v1/2025.acl-long.1424},
    pages = {29428--29442},
    isbn = {979-8-89176-251-0},
    abstract = {Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of social interactions. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose Class Distillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models. These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.},
    bibtex_show = {true},
    pdf = {wang2025class.pdf},
    %google_scholar_id = {}
}

@inproceedings{phi2024paying,
  title = {{Paying Attention to Deflections: Mining Pragmatic Nuances for Whataboutism Detection in Online Discourse}},
  author = "Phi, Khiem and Salek Faramarzi, Noushin and Wang, Chenlu and Banerjee, Ritwik",
  editor = "Ku, Lun-Wei and Martins, Andre and Srikumar, Vivek",
  booktitle = "Findings of the Association for Computational Linguistics: ACL 2024",
  month = {August},
  year = "2024",
  address = "Bangkok, Thailand",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2024.findings-acl.750/",
  doi = "10.18653/v1/2024.findings-acl.750",
  pages = "12628--12643",
  abstract = "Whataboutism, a potent tool for disrupting narratives and sowing distrust, remains under-explored in quantitative NLP research. Moreover, past work has not distinguished its use as a strategy for misinformation and propaganda from its use as a tool for pragmatic and semantic framing. We introduce new datasets from Twitter/X and YouTube, revealing overlaps as well as distinctions between whataboutism, propaganda, and the tu quoque fallacy. Furthermore, drawing on recent work in linguistic semantics, we differentiate the `what about' lexical construct from whataboutism. Our experiments bring to light unique challenges in its accurate detection, prompting the introduction of a novel method using attention weights for negative sample mining. We report significant improvements of 4% and 10% over previous state-of-the-art methods in our Twitter and YouTube collections, respectively.",
  bibtex_show={true},
  pdf={phi2024paying.pdf},
  code={https://github.com/KhiemPhi/wabt-det},
  selected={false},
  preview={true},
  %google_scholar_id = {uWQEDVKXjbEC},
}

@inproceedings{zuo2024from,
  author = {Zuo, Chaoyuan and Liu, Yishuang and Wang, Chenlu and Banerjee, Ritwik},
  title = {{From Claim to Evidence: Verifying Chinese Health Claims with Medical Literature}},
  year = {2024},
  month = {November},
  editor= {Wong, Derek F. and Wei, Zhongyu and Yang, Muyun},
  url = {https://doi.org/10.1007/978-981-97-9440-9_14},
  doi = {10.1007/978-981-97-9440-9_14},
  publisher="Springer Nature Singapore",
  address="Singapore",
  pages="171--183",
  abstract="Ensuring the accuracy of health claims in media is vital for public well-being, and evidence-based claim verification is critical in achieving this goal. However, identifying relevant biomedical literature as evidence for health claims is particularly challenging, especially within cross-genre and cross-lingual contexts. We propose an ad hoc information retrieval (IR) task to identify support for Chinese health claims obtained from Chinese news sources. We demonstrate the feasibility of such a task by presenting experiments on a novel dataset of pairs of Chinese health claims and English biomedical literature. We describe a two-step methodology comprising (i) a selection of the most relevant candidates from 764K research papers, and (ii) a final re-ranking of this selection. Our comprehensive experimental research demonstrates that incorporating domain-specific information significantly enhances retrieval accuracy and claim verification efficacy. This strategy is a major step toward improving the credibility of public health information dissemination and reducing the prevalence of falsehoods in health journalism.",
  isbn={978-981-97-9440-9},
  booktitle = {Natural Language Processing and Chinese Computing},
  bibtex_show={true},
  pdf={zuo2024from-nlpcc.pdf},
  selected={false},
  preview={true},
  %google_scholar_id = {l7t_Zn2s7bgC},
}

@article{koraishy2024use,
  author = {Koraishy, Farrukh M. and Wang, Chenlu and Banerjee, Ritwik and Kuperstein, Harry and Malick, Hamza and Tahir, Hira and Bano, Ruqiyya and Sakhuja, Priyal and Hajagos, Janos G.},
  title = {{Use of Natural Language Processing and Deep Learning to Analyze Kidney Ultrasound Reports and Their Correlation with CKD Diagnosis}},
  journal = {Journal of the American Society of Nephrology},
  year = {2024},
  volume = {35},
  number = {10S},
  month = {October},
  doi = {10.1681/ASN.20245jpx18zf},
  bibtex_show = {true},
  html = {https://journals.lww.com/jasn/fulltext/2024/10001/use_of_natural_language_processing_and_deep.2757.aspx},
  selected={false},
  preview={true},
  %google_scholar_id = {5Ul4iDaHHb8C},
}

@inproceedings{zuo2020querying,
  title = {{Querying Across Genres for Medical Claims in News}},
  author = "Zuo, Chaoyuan and Acharya, Narayan and Banerjee, Ritwik",
  editor = "Webber, Bonnie and Cohn, Trevor and He, Yulan and Liu, Yang",
  booktitle = "Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing (EMNLP)",
  month = {November},
  year = "2020",
  address = "Online",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/2020.emnlp-main.139/",
  doi = "10.18653/v1/2020.emnlp-main.139",
  pages = "1783--1789",
  abstract = "We present a query-based biomedical information retrieval task across two vastly different genres -- newswire and research literature -- where the goal is to find the research publication that supports the primary claim made in a health-related news article. For this task, we present a new dataset of 5,034 claims from news paired with research abstracts. Our approach consists of two steps: (i) selecting the most relevant candidates from a collection of 222k research abstracts, and (ii) re-ranking this list. We compare the classical IR approach using BM25 with more recent transformer-based models. Our results show that cross-genre medical IR is a viable task, but incorporating domain-specific knowledge is crucial.",
  bibtex_show = {true},
  pdf = {assets/pdf/zuo2020querying.pdf},
  code={https://github.com/chzuo/emnlp2020-cross-genre-IR},
  preview = {true},
  video = {https://slideslive.com/38939331/querying-across-geners-for-medical-claims-in-news},
  %google_scholar_id = {_xSYboBqXhAC}
}

@inproceedings{salekfaramarzi2023context,
    title = {{Context-aware Medication Event Extraction from Unstructured Text}},
    author = "Salek Faramarzi, Noushin and Patel, Meet and Bandarupally, Sai Harika and Banerjee, Ritwik",
    editor = "Naumann, Tristan and Ben Abacha, Asma and Bethard, Steven and Roberts, Kirk and Rumshisky, Anna",
    booktitle = "Proceedings of the 5th Clinical Natural Language Processing Workshop",
    month = {July},
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.clinicalnlp-1.11/",
    doi = "10.18653/v1/2023.clinicalnlp-1.11",
    pages = "86--95",
    abstract = "Accurately capturing medication history is crucial in delivering high-quality medical care. The extraction of medication events from unstructured clinical notes, however, is challenging because the information is presented in complex narratives. We address this challenge by leveraging the newly released Contextualized Medication Event Dataset (CMED) as part of our participation in the 2022 National NLP Clinical Challenges (n2c2) shared task. Our study evaluates the performance of various pretrained language models in this task. Further, we find that data augmentation coupled with domain-specific training provides notable improvements. With experiments, we also underscore the importance of careful data preprocessing in medical event detection.",
    bibtex_show = {true},
    pdf = {salekfaramarzi2023context.pdf},
    preview = {true},
    %google_scholar_id = {dshw04ExmUIC}
}

@inproceedings{zuo2021empirical,
    title = {{An Empirical Assessment of the Qualitative Aspects of Misinformation in Health News}},
    author = "Zuo, Chaoyuan and Zhang, Qi and Banerjee, Ritwik",
    editor = "Feldman, Anna and Da San Martino, Giovanni and Leberknight, Chris and Nakov, Preslav",
    booktitle = "Proceedings of the Fourth Workshop on NLP for Internet Freedom: Censorship, Disinformation, and Propaganda",
    month = {June},
    year = "2021",
    address = "Online",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.nlp4if-1.11/",
    doi = "10.18653/v1/2021.nlp4if-1.11",
    pages = "76--81",
    abstract = "The explosion of online health news articles runs the risk of the proliferation of low-quality information. Within the existing work on fact-checking, however, relatively little attention has been paid to medical news. We present a health news classification task to determine whether medical news articles satisfy a set of review criteria deemed important by medical experts and health care journalists. We present a dataset of 1,119 health news paired with systematic reviews. The review criteria consist of six elements that are essential to the accuracy of medical news. We then present experiments comparing the classical token-based approach with the more recent transformer-based models. Our results show that detecting qualitative lapses is a challenging task with direct ramifications in misinformation, but is an important direction to pursue beyond assigning True or False labels to short claims.",
    pdf = {zuo2021empirical.pdf},
    bibtex_show = {true},
    preview = {true},
    %google_scholar_id = {EUQCXRtRnyEC}
}

@inproceedings{saravani2021investigation,
    title = "{A}n {I}nvestigation into the {C}ontribution of {L}ocally {A}ggregated {D}escriptors to {F}igurative {L}anguage {I}dentification",
    author = "Saravani, Sina Mahdipour and Banerjee, Ritwik and Ray, Indrakshi",
    editor = "Sedoc, Jo{\~a}o and Rogers, Anna and Rumshisky, Anna and Tafreshi, Shabnam",
    booktitle = "Proceedings of the Second Workshop on Insights from Negative Results in NLP",
    month = {November},
    year = "2021",
    address = "Online and Punta Cana, Dominican Republic",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2021.insights-1.15/",
    doi = "10.18653/v1/2021.insights-1.15",
    pages = "103--109",
    abstract = "In natural language understanding, topics that touch upon figurative language and pragmatics are notably difficult. We probe a novel use of locally aggregated descriptors -- specifically, an architecture called NeXtVLAD -- motivated by its accomplishments in computer vision, achieve tremendous success in the FigLang2020 sarcasm detection task. The reported F1 score of 93.1\% is 14\% higher than the next best result. We specifically investigate the extent to which the novel architecture is responsible for this boost, and find that it does not provide statistically significant benefits. Deep learning approaches are expensive, and we hope our insights highlighting the lack of benefits from introducing a resource-intensive component will aid future research to distill the effective elements from long and complex pipelines, thereby providing a boost to the wider research community.",
    pdf = {saravani2021investigation.pdf},
    code={https://github.com/sinamps/nextvlad-for-nlp},
    bibtex_show = {true},
    preview = {true},
    video = {https://aclanthology.org/2021.insights-1.15.mp4},
    %google_scholar_id = {EUQCXRtRnyEC}
}

@inproceedings{banerjee2014keystroke,
    title = {{Keystroke Patterns as Prosody in Digital Writings: A Case Study with Deceptive Reviews and Essays}},
    author = "Banerjee, Ritwik and Feng, Song and Kang, Jun Seok and Choi, Yejin",
    editor = "Moschitti, Alessandro and Pang, Bo and Daelemans, Walter",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = {October},
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/D14-1155/",
    doi = "10.3115/v1/D14-1155",
    pages = "1469--1473",
    abstract = {In this paper, we explore the use of keyboard strokes as a means to access the real-time writing process of online authors, analogously to prosody in speech analysis, in the context of deception detection. We show that differences in keystroke patterns like editing maneuvers and duration of pauses can help distinguish between truthful and deceptive writing. Empirical results show that incorporating keystroke-based features lead to improved performance in deception detection in two different domains: online reviews and essays},
    pdf = {banerjee2014keystroke.pdf},
    bibtex_show = {true},
    preview = {true},
    %google_scholar_id = {j3f4tGmQtD8C}
}

@inproceedings{feng2012characterizing,
  title = {{Characterizing Stylistic Elements in Syntactic Structure}},
  author = "Feng, Song and Banerjee, Ritwik and Choi, Yejin",
  editor = "Tsujii, Jun{'}ichi and Henderson, James and Pa{\c{s}}ca, Marius",
  booktitle = "Proceedings of the 2012 Joint Conference on Empirical Methods in Natural Language Processing and Computational Natural Language Learning",
  month = {July},
  year = "2012",
  address = "Jeju Island, Korea",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/D12-1139/",
  pages = "1522--1533",
  abstract = {Much of the writing styles recognized in rhetorical and composition theories involve deep syntactic elements. However, most previous research for computational stylometric analysis has relied on shallow lexico-syntactic patterns. Some very recent work has shown that PCFG models can detect distributional difference in syntactic styles, but without offering much insights into exactly what constitute salient stylistic elements in sentence structure characterizing each authorship. In this paper, we present a comprehensive exploration of syntactic elements in writing styles, with particular emphasis on interpretable characterization of stylistic elements. We present analytic insights with respect to the authorship attribution task in two different domains.},
  bibtex_show = {true},
  preview = {true},
  pdf = {feng2012characterizing.pdf},
  %google_scholar_id = {u-x6o8ySG0sC}
}

@inproceedings{feng2012syntactic,
  title = {{Syntactic Stylometry for Deception Detection}},
  author = "Feng, Song and Banerjee, Ritwik and Choi, Yejin",
  editor = "Li, Haizhou and Lin, Chin-Yew and Osborne, Miles and Lee, Gary Geunbae and Park, Jong C.",
  booktitle = "Proceedings of the 50th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
  month = {July},
  year = "2012",
  address = "Jeju Island, Korea",
  publisher = "Association for Computational Linguistics",
  url = "https://aclanthology.org/P12-2034/",
  pages = "171--175",
  abstract = {Most previous studies in computerized deception detection have relied only on shallow lexico-syntactic patterns. This paper investigates syntactic stylometry for deception detection, adding a somewhat unconventional angle to prior literature. Over four different datasets spanning from the product review to the essay domain, we demonstrate that features driven from Context Free Grammar (CFG) parse trees consistently improve the detection performance over several baselines that are based only on shallow lexico-syntactic features. Our results improve the best published result on the hotel review data (<a href="https://arxiv.org/abs/1107.4557">Ott et al., 2011</a>) reaching 91.2% accuracy with 14% error reduction.},
  bibtex_show = {true},
  preview = {true},
  pdf = {feng2012syntactic.pdf},
  %google_scholar_id = {u5HHmVD_uO8C}
}


@inproceedings{banerjee2014automated,
  author = {Banerjee, Ritwik and Choi, Yejin and Piyush, Gaurav and Naik, Ameya and Ramakrishnan, I. V.},
  title = {{Automated Suggestion of Tests for Identifying Likelihood of Adverse Drug Events}},
  year = {2014},
  isbn = {9781479957019},
  publisher = {IEEE Computer Society},
  address = {Verona, Italy},
  abstract = {Adverse drug events (ADE) caused by use, misuse or sudden discontinuation of medications trigger hospital emergency room visits. Information about a wide range of drugs and associated ADEs is provided in online drug databases in the form of narrative texts. Even though some ADEs can be detected by observable symptoms, several others can only be confirmed by laboratory tests. In this paper, we present a system that provides automated suggestion of tests to identify the likelihood of ADEs. Given a patient's medications and an optional list of signs and symptoms, our system automatically produces the laboratory tests needed to confirm possible ADEs associated with these drugs. The basis of our application is to map clinical symptoms to medical problems and laboratory tests. Towards that, we use template-based extraction and shallow parsing techniques from natural language processing to extract information from narrative texts. We employ relevance ranking measures to establish correspondence between the tests and ADEs. Our evaluation based on a sample set of 40 drugs shows that this system achieves relatively high sensitivity.},
  booktitle = {Proceedings of the 2014 IEEE International Conference on Healthcare Informatics},
  pages = {170--175},
  series = {ICHI '14},
  bibtex_show = {true},
  preview = {true},
  pdf = {banerjee2014automated.pdf},
  %google_scholar_id = {4JMBOYKVnBMC}
}


@inproceedings{banerjee2015patient,
  author={Banerjee, Ritwik and Ramakrishnan, I.V. and Henry, Mark and Perciavalle, Matthew},
  booktitle={2015 International Conference on Healthcare Informatics}, 
  title={{Patient Centered Identification, Attribution, and Ranking of Adverse Drug Events}}, 
  year={2015},
  pages={18--27},
  doi={10.1109/ICHI.2015.8},
  publisher = {IEEE Computer Society},
  address={Dallas, TX, USA},
  series={ICHI '15'},
  url={https://doi.org/10.1109/ICHI.2015.8},
  abstract={Adverse drug events (ADEs) trigger a high number of hospital emergency room (ER) visits. Information about ADEs is often available in online drug databases in the form of narrative texts, and serves as the physician's primary reference point for ADE attribution and diagnosis. Manually reviewing these narratives, however, is an error prone and time consuming process, especially due to the prevalence of polypharmacy. So ER health care providers, especially given the heavy volume of traffic in ERs, often either skip this step or at best do it rather perfunctorily. This causes ADEs to be missed or misdiagnosed, often leading to extensive and unnecessary testing and treatment, including hospitalization. In this paper, we present a system that automates the detection of ADEs and provides a list of suspect drugs, ranked by their likelihood of causing the patient's complaints and symptoms. The input data, i.e., Medications and complaints, are obtained from triage notes that often contain descriptive language. Our application utilizes heterogeneous information sources (including drug databases) to refine and transform these descriptions as well as the online database narratives using a natural language processing (NLP) pipeline. We then employ ranking measures to establish correspondence between the complaints and the medications. Our preliminary evaluation based on actual ER cases demonstrates that this system achieves high precision and recall.},
  bibtex_show = {true},
  preview = {true},
  pdf = {banerjee2015patient.pdf},
  award_name={Best Technical Paper Award},
  award={This paper on an improved patient care system, won the IEEE International Conference on Healthcare Informatics (ICHI) 2015 <strong>Best Technical Paper Award</strong>. This system was the outcome of  collaborative research  between Dr. Ritwik Banerjee and Dr. I.V. Ramakrishnan from Computer Science and Dr. Mark Henry, Chair of Emergency Medicine and Matthew Perciavalle, Clinical Pharmacist, from Stony Brook School of Medicine.},
  %google_scholar_id = {bEWYMUwI8FkC}
}

@inproceedings{banerjee2015pam,
  author={Banerjee, Ritwik and Razaghpanah, Abbas and Chiang, Luis and Mishra, Akassh and Sekar, Vyas and Choi, Yejin and Gill, Phillipa},
  title={{Internet Outages, the Eyewitness Accounts: Analysis of the Outages Mailing List}},
  booktitle={{Passive and Active Measurement: 16th International Conference}},
  editor="Mirkovic, Jelena and Liu, Yong",
  year={2015},
  volume={8995},
  pages={206--219},
  publisher="Springer International Publishing",
  address="Cham",
  abstract="Understanding network reliability and outages is critical to the ``health'' of the Internet infrastructure. Unfortunately, our ability to analyze Internet outages has been hampered by the lack of access to public information from key players. In this paper, we leverage a somewhat unconventional dataset to analyze Internet reliability---the outages mailing list. The mailing list is an avenue for network operators to share information and insights about widespread outages. Using this unique dataset, we perform a first-of-its-kind longitudinal analysis of Internet outages from 2006 to 2013 using text mining and natural language processing techniques. We observe several interesting aspects of Internet outages: a large number of application and mobility issues that impact users, a rise in content, mobile issues, and discussion of large-scale DDoS attacks in recent years.",
  isbn="978-3-319-15509-8",
  location="New York, USA",
  url={https://doi.org/10.1007/978-3-319-15509-8_16},
  bibtex_show = {true},
  preview = {true},
  pdf = {banerjee2015internet.pdf},
  %google_scholar_id = {iH-uZ7U-co4C}
}


@inproceedings{zuo2018hybrid,
  author={Zuo, Chaoyuan and Karakas, Ayla and Banerjee, Ritwik},
  title={{A Hybrid Recognition System for Check-worthy Claims Using Heuristics and Supervised Learning}},
  booktitle={CLEF 2018 Working Notes. Working Notes of CLEF 2018 - Conference and Labs of the Evaluation Forum},
  series={{CEUR} Workshop Proceedings},
  publisher={CEUR-WS.org},
  editor={Cappellato, Linda and Ferro, Nicola and Nie, Jian-Yun and Soulier, Laure},
  address={Avignon, France},
  year={2018},
  url = {https://ceur-ws.org/Vol-2125/paper_143.pdf},
  abstract={In recent years, the speed at which information disseminates has received an alarming boost from the pervasive usage of social media. To the detriment of political and social stability, this has also made it easier to quickly spread false claims. Due to the sheer volume of information, manual fact-checking seems infeasible, and as a result, computational approaches have been recently explored for automated fact-checking. In spite of the recent advancements in this direction, the critical step of recognizing and prioritizing statements worth fact-checking has received little attention. In this paper, we propose a hybrid approach that combines simple heuristics with supervised machine learning to identify claims made in political debates and speeches, and provide a mechanism to rank them in terms of their "check-worthiness". The viability of our method is demonstrated by evaluations on the English language dataset as part of the Check-worthiness task of the CLEF-2018 Fact Checking Lab.},
  %google_scholar_id = {4OULZ7Gr8RgC},
  bibtex_show={true},
  pdf={zuo2018hybrid-clef.pdf},
  selected={false},
  preview={true}
}

@inproceedings{zuo2019tocheck,
  author="Zuo, Chaoyuan and Karakas, Ayla Ida and Banerjee, Ritwik",
  editor="Crestani, F. and Braschler, M. and Savoy, J. and Rauber, A. and M{\"u}ller, H. and Losada, D. E. and Heinatz B{\"u}rki, G. and Cappellato, L. and Ferro, N.",
  title={{To Check or Not to Check: Syntax, Semantics, and Context in the Language of Check-Worthy Claims}},
  booktitle="Experimental IR Meets Multilinguality, Multimodality, and Interaction",
  year="2019",
  publisher="Springer",
  address="Cham",
  pages="271--283",
  doi = {10.1007/978-3-030-28577-7_23},
  url = {https://link.springer.com/chapter/10.1007/978-3-030-28577-7_23},
  note = {Invited Paper},
  abstract = {As the spread of information has received a compelling boost due to pervasive use of social media, so has the spread of misinformation. The sheer volume of data has rendered the traditional methods of expert-driven manual fact-checking largely infeasible. As a result, computational linguistics and data-driven algorithms have been explored in recent years. Despite this progress, identifying and prioritizing what needs to be checked has received little attention. Given that expert-driven manual intervention is likely to remain an important component of fact-checking, especially in specific domains (e.g., politics, environmental science), this identification and prioritization is critical. A successful algorithmic ranking of “check-worthy” claims can help an expert-in-the-loop fact-checking system, thereby reducing the expert’s workload while still tackling the most salient bits of misinformation. In this work, we explore how linguistic syntax, semantics, and the contextual meaning of words play a role in determining the check-worthiness of claims. Our preliminary experiments used explicit stylometric features and simple word embeddings on the English language dataset in the Check-worthiness task of the CLEF-2018 Fact-Checking Lab, where our primary solution outperformed the other systems in terms of the mean average precision, R-precision, reciprocal rank, and precision at k for multiple values k. Here, we present an extension of this approach with more sophisticated word embeddings and report further improvements in this task.},
  award_name={Invited Paper},
  award={This paper was invited to CLEF 2019 as one of the "Best of the Labs".},
  %google_scholar_id = {f2IySw72cVMC},
  bibtex_show={true},
  pdf={zuo2019tocheck-clef.pdf},
  selected={false},
  preview={true}
}

@inproceedings{zuo2019style,
  author={Zuo, Chaoyuan and Zhao, Yu and Banerjee, Ritwik},
  title={{Style Change Detection with Feed-forward Neural Networks}},
  booktitle = {CLEF 2019 Working Notes. Working Notes of CLEF 2019 - Conference and Labs of the Evaluation Forum},
  series={{CEUR} Workshop Proceedings},
  volume={2380},
  publisher={CEUR-WS.org},
  editor={Cappellato, Linda and Ferro, Nicola and Losada, David E. and Müller, Henning},
  address={Lugano, Switzerland},
  year={2019},
  url={https://ceur-ws.org/Vol-2380/paper_229.pdf},
  abstract={The majority of previous authorship attribution studies mainly focus on a dataset of documents (or parts of documents) with labeled authorship. This scenario, however, is not applicable to documents written by more than one author. Detecting the authorship switches within multi-author documents has been shown to be a challenging task in previous PAN tasks. A simplified version of the style change task is thus organized by PAN 2019, which aims at identifying the number of authors in a given document. To this end, we present a system consisting of two modules, one for distinguishing the single-author documents from the multiauthor documents and the other for determining the exact number of authors in the multi-author documents.},
  %google_scholar_id = {pyW8ca7W8N0C},
  bibtex_show={true},
  pdf={zuo2019style-clef.pdf},
  code={https://github.com/chzuo/PAN_2019},
  selected={false},
  preview={true}
}

@inproceedings{banerjee2021diagnosis,
  author={Banerjee, Ritwik and Ray, Indrakshi},
  booktitle={2021 IEEE Third International Conference on Cognitive Machine Intelligence (CogMI)}, 
  title={{Diagnosis, Prevention, and Cure for Misinformation *}},
  year={2021},
  pages={156--162},
  publisher={IEEE},
  address={Atlanta, GA, USA},
  url={https://doi.org/10.1109/CogMI52975.2021.00028},
  doi={10.1109/CogMI52975.2021.00028},
  abstract={Misinformation has become a widespread problem in contemporary society, harming the most vulnerable sections in particular. It incurs a high cost on everyone - socially, politically, and even financially. Much of the progress made in tackling misinformation, however, has only offered relatively simple solutions by taking a narrow view of “misinformation” itself. We discuss the various nuances of the term, illustrating the technical difficulties that arise from them, and propose a multiplicity of context-sensitive modeling approaches that may prove to be fruitful in addressing these difficulties. A tremendous amount of work remains to be done to ensure our inoculation from misinformation and its harmful effects, and much of this work, we argue, requires collaborative effort across disciplines. It is our hope that this article serves as a call to further such research in this field.},
  %google_scholar_id = {CHSYGLWDkRkC},
  bibtex_show={true},
  pdf={https://par.nsf.gov/servlets/purl/10341221},
  selected={false},
  preview={true},
  annotation={* Vision Paper}
}

@inproceedings{salekfaramarzi2022combining,
  author={Salek Faramarzi, Noushin and Dara, Akanksha and Banerjee, Ritwik},
  booktitle={2022 IEEE 10th International Conference on Healthcare Informatics (ICHI)}, 
  title={{Combining Attention-based Models with the MeSH Ontology for Semantic Textual Similarity in Clinical Notes}}, 
  year={2022},
  pages={74--83},
  publisher = {IEEE Computer Society},
  address={Rochester, MN, USA},
  url={https://doi.org/10.1109/ICHI54592.2022.00023},
  doi={10.1109/ICHI54592.2022.00023},
  abstract={In this study, we present several transformer-based models as well as traditional machine learning methods to detect semantic textual similarity (STS) in clinical notes. We investigate transformer models pretrained on general English as well as clinical notes, and use generic English STS datasets as a supplemental corpus to clinical notes data. Our work is based on the 2019 National NLP Clinical Challenge (n2c2). We identify and annotate six types of sentences in the clinical notes corpus, and report an ensemble method that combines attention-based contextualized embeddings with a similarity score based on the MeSH ontology obtained by computing least common ancestors of clinical terms. Our approach does not need additional clinical data for model training, while still achieving comparable Pearson's correlation coefficient of 0.901.},
  %google_scholar_id = {nb7KW1ujOQ8C},
  bibtex_show={true},
  pdf={salekfaramarzi2022combining.pdf},
  selected={false},
  preview={true}
}

@inproceedings{salekfaramarzi2023claim,
  author = {Salek Faramarzi, Noushin and Hashemi Chaleshtori, Fateme and Shirazi, Hossein and Ray, Indrakshi and Banerjee, Ritwik},
  title = {{Claim Extraction and Dynamic Stance Detection in COVID-19 Tweets}},
  year = {2023},
  isbn = {9781450394192},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  url = {https://doi.org/10.1145/3543873.3587643},
  doi = {10.1145/3543873.3587643},
  abstract = {The information ecosystem today is noisy, and rife with messages that contain a mix of objective claims and subjective remarks or reactions. Any automated system that intends to capture the social, cultural, or political zeitgeist, must be able to analyze the claims as well as the remarks. Due to the deluge of such messages on social media, and their tremendous power to shape our perceptions, there has never been a greater need to automate these analyses, which play a pivotal role in fact-checking, opinion mining, understanding opinion trends, and other such downstream tasks of social consequence. In this noisy ecosystem, not all claims are worth checking for veracity. Such a check-worthy claim, moreover, must be accurately distilled from subjective remarks surrounding it. Finally, and especially for understanding opinion trends, it is important to understand the stance of the remarks or reactions towards that specific claim. To this end, we introduce a COVID-19 Twitter dataset, and present a three-stage process to (i) determine whether a given Tweet is indeed check-worthy, and if so, (ii) which portion of the Tweet ought to be checked for veracity, and finally, (iii) determine the author’s stance towards the claim in that Tweet, thus introducing the novel task of topic-agnostic stance detection.},
  booktitle = {Companion Proceedings of the ACM Web Conference 2023},
  pages = {1059--1068},
  location = {Austin, TX, USA},
  series = {WWW '23 Companion},
  %google_scholar_id = {KxtntwgDAa4C},
  bibtex_show={true},
  pdf={salekfaramarzi2023claim.pdf},
  selected={false},
  preview={true}
}

@incollection{zuo2023cross,
  author={Zuo, Chaoyuan and Wang, Chenlu and Banerjee, Ritwik},
  title={{Cross-genre Retrieval for Information Integrity: A COVID-19 Case Study}},
  booktitle={Advanced Data Mining and Applications: 19th International Conference},
  year={2023},
  series={ADMA '23 (Lecture Notes in Computer Science)},
  volume={14180},
  pages={495--509},
  editor={Yang, Xiaochun and Suhartanto, Heru and Wang, Guoren and Wang, Bin and Jiang, Jing and Li, Bing and Zhu, Huaijie and Cui, Ningning},
  publisher={Springer Nature Switzerland},
  address={Cham},
  url={https://doi.org/10.1007/978-3-031-46677-9_34},
  doi={10.1007/978-3-031-46677-9_34},
  abstract={Ubiquitous communication on social media has led to a rapid increase in the proliferation of unreliable information. Its ill-effects have perhaps been seen most obviously during the COVID-19 pandemic, and have rightfully raised concerns about the integrity of shared information. This work focuses on derivative Twitter posts (tweets), i.e., posts that re-transmit preexisting content. We acknowledge that a considerable number of such tweets do not provide a source of information, which undoubtedly undermines the integrity of the information and poses difficulties in fact-checking. To address this concern, we propose an ad hoc information retrieval (IR) task to identify the support for claims made in tweets from reputable news outlets. We demonstrate the feasibility of such cross-genre IR by presenting experiments on a COVID-19 dataset of 11K pairs of tweets and news articles. We describe a two-step methodology: (i) selecting the most relevant candidates from 57K pandemic-related news articles, and (ii) a final re-ranking of this selection. Our method achieves significant improvements over the classical token-based approach using BM25 as well as a state-of-the-art transformer-based language model pretrained on COVID-19 tweets. Our findings demonstrate the viability of cross-genre IR across news and social media in safeguarding the integrity of information disseminated through social media.},
  %google_scholar_id = {UxriW0iASnsC},
  bibtex_show={true},
  pdf={zuo2023cross-adma.pdf},
  selected={false},
  preview={true}
}

@article{zuo2022seeing,
  author = {Zuo, Chaoyuan and Banerjee, Ritwik and Chaleshtori, Fateme Hashemi and Shirazi, Hossein and Ray, Indrakshi},
  title = {{Seeing Should Probably Not Be Believing: The Role of Deceptive Support in COVID-19 Misinformation on Twitter}},
  year = {2022},
  issue_date = {March 2023},
  month={December},
  publisher = {Association for Computing Machinery},
  address = {New York, NY, USA},
  volume = {15},
  number = {1},
  issn = {1936-1955},
  url = {https://doi.org/10.1145/3546914},
  doi = {10.1145/3546914},
  abstract = {With the spread of the SARS-CoV-2, enormous amounts of information about the pandemic are disseminated through social media platforms such as Twitter. Social media posts often leverage the trust readers have in prestigious news agencies and cite news articles as a way of gaining credibility. Nevertheless, it is not always the case that the cited article supports the claim made in the social media post. We present a cross-genre ad hoc pipeline to identify whether the information in a Twitter post (i.e., a “Tweet”) is indeed supported by the cited news article. Our approach is empirically based on a corpus of over 46.86 million Tweets and is divided into two tasks: (i) development of models to detect Tweets containing claim and worth to be fact-checked and (ii) verifying whether the claims made in a Tweet are supported by the newswire article it cites. Unlike previous studies that detect unsubstantiated information by post hoc analysis of the patterns of propagation, we seek to identify reliable support (or the lack of it) before the misinformation begins to spread. We discover that nearly half of the Tweets (43.4\%) are not factual and hence not worth checking—a significant filter, given the sheer volume of social media posts on a platform such as Twitter. Moreover, we find that among the Tweets that contain a seemingly factual claim while citing a news article as supporting evidence, at least 1\% are not actually supported by the cited news and are hence misleading.},
  journal = {Journal of Data and Information Quality},
  articleno = {9},
  %google_scholar_id = {P5F9QuxV20EC},
  bibtex_show={true},
  pdf={zuo2022seeing-journal.pdf},
  selected={false},
  preview={true}
}

@article{zuo2022beyond,
  author = {Zuo, Chaoyuan and Mathur, Kritik and Kela, Dhruv and Salek Faramarzi, Noushin and Banerjee, Ritwik},
  title = {Beyond belief: a cross-genre study on perception and validation of health information online},
  journal = {International Journal of Data Science and Analytics},
  year = {2022},
  month = {February},
  pages = {299--314},
  volume = {13},
  issue = {4},
  issn = {2364-415X},
  url={https://doi.org/10.1007/s41060-022-00310-7},
  doi={10.1007/s41060-022-00310-7},
  abstract={Natural language undergoes significant transformation from the domain of specialized research to general news intended for wider consumption. This transition makes the information vulnerable to misinterpretation, misrepresentation, and incorrect attribution, all of which may be difficult to identify without adequate domain knowledge and may exist even in the presence of explicit citations. Moreover, newswire articles seldom provide a precise correspondence between a specific claim and its origin, making it harder to identify which claims, if any, reflect the original findings. For instance, an article stating "Flagellin shows therapeutic potential with H3N2, known as Aussie Flu." contains two claims ("Flagellin ... H3N2," and "H3N2, known as Aussie Flu") that may be true or false independent of each other, and it is prima facie unclear which claims, if any, are supported by the cited research. We build a dataset of sentences from medical news along with the sources from peer-reviewed medical research journals they cite. We use these data to study what a general reader perceives to be true, and how to verify the scientific source of claims. Unlike existing datasets, this captures the metamorphosis of information across two genres with disparate readership and vastly different vocabularies and presents the first empirical study of health-related fact-checking across them.},
  %google_scholar_id = {xtRiw3GOFMkC},
  bibtex_show={true},
  pdf={zuo2022beyond-journal.pdf},
  html={https://pmc.ncbi.nlm.nih.gov/articles/PMC8807956/},
  code={https://github.com/chzuo/jdsa_cross_genre_validation},
  selected={false},
  preview={true}
}
